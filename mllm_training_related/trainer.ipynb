{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Union, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedModel, PretrainedConfig, AutoTokenizer, AutoModelForCausalLM,AutoProcessor, AutoModel, \\\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizers = AutoTokenizer.from_pretrained(\"./base_models/llm_model_qwen2.5_1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLLMConfig(PretrainedConfig):\n",
    "    model_type = \"mllm\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/base_models/llm_model_qwen2.5_1.5b',\n",
    "        vision_model_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/base_models/vision_model_siglip_16_224',\n",
    "        image_pad_num = 81,\n",
    "        freeze_vision_model = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.llm_model_path = llm_model_path\n",
    "        self.vision_model_path = vision_model_path\n",
    "        self.image_pad_num = image_pad_num\n",
    "        self.freeze_vision_model = freeze_vision_model\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLLM(PreTrainedModel):\n",
    "    config_class = MLLMConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.vision_model = AutoModel.from_pretrained(self.config.vision_model_path)\n",
    "        self.processor = AutoProcessor.from_pretrained(self.config.vision_model_path)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained(self.config.llm_model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.llm_model_path)\n",
    "        self.linear1 = nn.Linear(self.vision_model.config.vision_config.hidden_size*9, self.llm_model.config.hidden_size)\n",
    "        self.linear2 = nn.Linear(self.llm_model.config.hidden_size, self.llm_model.config.hidden_size)\n",
    "        if self.config.freeze_vision_model:\n",
    "            for param in self.vision_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        pixel_values,\n",
    "        labels,\n",
    "        attention_mask=None  \n",
    "    ):\n",
    "        \"\"\"\n",
    "        模型的前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        - input_ids (torch.Tensor): 输入的 token 索引张量。\n",
    "        - attention_mask (torch.Tensor): 注意力掩码张量。\n",
    "        - pixel_values (torch.Tensor): 图像像素值张量。\n",
    "        - labels (torch.Tensor): 标签张量。\n",
    "\n",
    "        返回:\n",
    "        - CausalLMOutputWithPast: 包含 logits 和损失的输出对象。\n",
    "        \"\"\"\n",
    "        # 通过视觉模型获取图像的嵌入表示\n",
    "        vision_embedding = self.vision_model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
    "        # 通过语言模型获取文本的嵌入表示\n",
    "        text_embedding = self.llm_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "        batch_size, image_tokens, embedding_dim_size = vision_embedding.shape\n",
    "        vision_embedding = vision_embedding.view(batch_size,-1,embedding_dim_size*9)\n",
    "        # 对视觉嵌入进行线性变换和激活函数处理，得到图像特征\n",
    "        image_features = self.linear2(F.silu(self.linear1(vision_embedding)))\n",
    "        text_embedding = text_embedding.to(image_features.dtype)\n",
    "\n",
    "        # 将图像特征与文本嵌入合并\n",
    "        inputs_embedding = self.merge_input_ids_with_image_features(image_features, text_embedding, input_ids)\n",
    "        # 将合并后的嵌入输入到语言模型中，获取输出\n",
    "        outputs = self.llm_model(inputs_embeds=inputs_embedding, attention_mask=attention_mask)\n",
    "        # 从输出中提取 logits\n",
    "        logits = outputs[0]\n",
    "        # 初始化损失为 None\n",
    "        loss = None\n",
    "        # 如果有标签，则计算损失\n",
    "        if labels is not None:\n",
    "            # 创建交叉熵损失函数\n",
    "            loss_func = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "            # 计算 logits 和标签之间的交叉熵损失\n",
    "            loss = loss_func(\n",
    "                logits.view(-1, logits.size(-1)), labels.view(-1).to(logits.device)\n",
    "            )\n",
    "        # 返回包含 logits 和损失的输出对象\n",
    "        return CausalLMOutputWithPast(logits=logits,loss=loss)\n",
    "    \n",
    "    \n",
    "    def merge_input_ids_with_image_features(self, image_features, inputs_embedding, input_ids):\n",
    "        num_images, num_image_pathes, embedding_dim_size = image_features.shape\n",
    "        batch_indices, image_indices = torch.where(input_ids == self.tokenizer('<|image_pad|>')['input_ids'][0])\n",
    "        inputs_embedding[batch_indices, image_indices]  = image_features.view(-1, embedding_dim_size)\n",
    "        return inputs_embedding\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你叫Flash,你是为一位专门为Brench服务的多模态AI助手\"\n",
    "}\n",
    "class PretrainedDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, config):\n",
    "        self.config = config\n",
    "        self.images_path = images_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.llm_model_path)\n",
    "        self.processor = AutoProcessor.from_pretrained(self.config.vision_model_path)\n",
    "\n",
    "        with open(self.annotations_path, 'r', encoding='utf-8') as f:\n",
    "            self.processor_data = json.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processor_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_sample = self.processor_data[index]\n",
    "        try:\n",
    "            image_file_name = data_sample['image']\n",
    "            conversations = data_sample['conversations']\n",
    "            image = Image.open(os.path.join(self.images_path, image_file_name)).convert('RGB')\n",
    "            pixel_values = self.processor(text=None, images=image)['pixel_values']\n",
    "            user_prompt = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": conversations[0]['value']\n",
    "            }\n",
    "            query_text = [system_prompt, user_prompt]\n",
    "            query_input = self.tokenizer.apply_chat_template(\n",
    "                query_text,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            ).replace('<image>','<|image_pad|>'*self.config.image_pad_num)\n",
    "            response_text = conversations[1]['value'] + self.tokenizer.eos_token\n",
    "            query_input_ids = self.tokenizer(query_input)['input_ids']\n",
    "            response_input_ids = self.tokenizer(response_text)['input_ids']\n",
    "            input_ids = query_input_ids + response_input_ids\n",
    "            labels = [self.tokenizer.pad_token_id] * len(query_input_ids) + response_input_ids\n",
    "            input_ids = input_ids[:-1]\n",
    "            labels = labels[1:]\n",
    "        except:\n",
    "            default_image = Image.new('RGB',(224,224),color='white')\n",
    "            pixel_values = self.processor(text=None, images=default_image)['pixel_values']\n",
    "            user_prompt = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":\"这张图片描述的内容是什么\\n<image>\"\n",
    "            }\n",
    "            query_text = [system_prompt, user_prompt]\n",
    "            query_input = self.tokenizer.apply_chat_template(\n",
    "                query_text,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            ).replace('<image>','<|image_pad|>'*self.config.image_pad_num)\n",
    "            response_text = \"图片内容为空，无法生成相关的回复\\n\" + self.tokenizer.eos_token\n",
    "            query_input_ids = self.tokenizer(query_input)['input_ids']\n",
    "            response_input_ids = self.tokenizer(response_text)['input_ids']\n",
    "            input_ids = query_input_ids + response_input_ids\n",
    "            labels = [self.tokenizer.pad_token_id] * len(query_input_ids) + response_input_ids\n",
    "            input_ids = input_ids[:-1]\n",
    "            labels = labels[1:]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'pixel_values': pixel_values\n",
    "        }\n",
    "            \n",
    "class  DatasetCollator: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.llm_model_path)\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str,Any]])->Dict[str,torch.Tensor]:\n",
    "        max_length = max(len(feature['input_ids']) for feature in features)\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        pixel_values = []\n",
    "        for feature in features:\n",
    "            input_ids.append(feature['input_ids'] + [self.tokenizer.pad_token_id] * (max_length - len(feature['input_ids'])))\n",
    "            labels.append(feature['labels'] + [self.tokenizer.pad_token_id] * (max_length - len(feature['labels'])))\n",
    "            pixel_values.append(feature['pixel_values'])\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'pixel_values': torch.cat(pixel_values, dim=0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MLLMConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/pretrained_data/LLaVA-CC3M-Pretrain-595K/images'\n",
    "annotations_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/pretrained_data/LLaVA-CC3M-Pretrain-595K/chat-translated.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dataset =  PretrainedDataset(images_path,annotations_path,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'pixel_values'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLLMConfig {\n",
       "  \"freeze_vision_model\": false,\n",
       "  \"image_pad_num\": 81,\n",
       "  \"llm_model_path\": \"/mnt/bn/brench-lq1/mllm_self_training/mllm_building/base_models/llm_model_qwen2.5_1.5b\",\n",
       "  \"model_type\": \"mllm\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"vision_model_path\": \"/mnt/bn/brench-lq1/mllm_self_training/mllm_building/base_models/vision_model_siglip_14_384\"\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/base_models/llm_model_qwen2.5_1.5b'\n",
    "vision_model_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/base_models/vision_model_siglip_14_384'\n",
    "image_pad_num = 81\n",
    "config = MLLMConfig(\n",
    "    llm_model_path = llm_model_path,\n",
    "    vision_model_path = vision_model_path,\n",
    "    image_pad_num = image_pad_num\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLLM(\n",
      "  (vision_model): SiglipModel(\n",
      "    (text_model): SiglipTextTransformer(\n",
      "      (embeddings): SiglipTextEmbeddings(\n",
      "        (token_embedding): Embedding(32000, 1152)\n",
      "        (position_embedding): Embedding(64, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (self_attn): SiglipSdpaAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      (head): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "    )\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(729, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (self_attn): SiglipSdpaAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      (head): SiglipMultiheadAttentionPoolingHead(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
      "        )\n",
      "        (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (llm_model): Qwen2ForCausalLM(\n",
      "    (model): Qwen2Model(\n",
      "      (embed_tokens): Embedding(151936, 1536)\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2SdpaAttention(\n",
      "            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "            (rotary_emb): Qwen2RotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      (rotary_emb): Qwen2RotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=10368, out_features=1536, bias=True)\n",
      "  (linear2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      ")\n",
      "模型参数量为：896248114\n"
     ]
    }
   ],
   "source": [
    "model = MLLM(config).cuda()\n",
    "print(model)\n",
    "print(f'模型参数量为：{sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/pretrained_data/LLaVA-CC3M-Pretrain-595K/images'\n",
    "annotations_path = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/pretrained_data/LLaVA-CC3M-Pretrain-595K/chat-translated.json'\n",
    "output_dir = '/mnt/bn/brench-lq1/mllm_self_training/mllm_building/pretrained_model_save' \n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    do_train=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=1,\n",
    "    report_to='tensorboard',\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/bn/brench-lq1/miniconda3/envs/mllm/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/mnt/bn/brench-lq1/miniconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='46510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    7/46510 00:10 < 27:27:25, 0.47 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.610700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=PretrainedDataset(images_path, annotations_path, config),\n",
    "    data_collator=DatasetCollator(config) \n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "trainer.save_model(output_dir)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
